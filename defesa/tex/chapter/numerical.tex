The goal of this chapter is to show in practice the results of the algorithms for MCE and MCER proposed by us. We first give some implementation details, then we start discuss the solutions obtained for instances proposed in past works, and finally we propose some new instances with the intention of finding the limits of our algorithms.

\section{Implementation}

All the algorithms were implemented using the C++ language, with compiler g++ (G++ 6.3.0). To activate the optimization of compilation we used the -O4 flag.. All the experiments were run in a computer with the following specification:
\begin{itemize}
	\item CPU Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz;
	\item 16Gib of RAM memory;
	\item Linux Operating System: Debian 4.19.5.
\end{itemize}
\subsection{Determining the eigenvalues of a matrix}

In \autoref{algoritmo:e3p}, we assumed that a procedure which returns every eigenvalue of a given square matrix was available. In practice, we used the very famous linear algebra package LAPACK (see \citeonline{lapack} for more details).
LAPACK is a library for the FORTRAN programming language. However, its routines can be made available in a C/C++ environment by simply adding the -llapack linking flag to the compilation. The only remarks, though, are that FORTAN represents matrices in a column-major fashion, and receives parameters only by reference. Therefore, matrices must be transposed before being passed to a routine, and every parameter must receive a pointer to a variable containing its value.

LAPACK offers a routine called ZGEEV that computes every eigenvalue of a complex matrix by using an implementation of the QR algorithm. 
This routine optionally can also be asked to compute the right or left eigenvectors depending on two of its parameters. 
ZGEEV receives in total $14$ parameters, with $4$ of them being used for output. We show a brief description of them in \autoref{tab:zgeev} along with the specification of the value we set each parameter in our implementation.
%\renewcommand{\arraystretch}{1.1}

\begin{table}[H]\label{tab:zgeev}
	\begin{center}
	\begin{tabular}{|c|m{18em}|m{8em}|}
		
		\hline
		\textbf{Parameter} & \multicolumn{1}{c|}{\textbf{Description}} & \multicolumn{1}{c|}{\textbf{Value}}\\
		\hline
		JOBVL&  Indicates whether to compute the left eigenvalues&  'N' (no eigenvectors should be computed)\\
		\hline
		JOBVR&  Indicates whether to compute the right eigenvalues&  'N' (no eigenvectors should be computed)\\
		\hline
		N    &  Order of matrix A &  6\\
		\hline
		A    &  The square matrix whose eigenvalues are to be computed & The companion matrix \\
		\hline
		LDA  &  Leading dimension of A & 6 \\
		\hline
		W    &  The eigenvalues output array &  A complex array of size 6\\
		\hline
		VL   &  The left eigenvectors output array & A complex array of size 1 \\
		\hline
		LDVL &  Leading dimension of VL&  1\\
		\hline
		VR   &  The right eigenvectors output array&  A complex array of size 1\\
		\hline
		LDVR &  Leading dimension of VR&  1\\
		\hline
		WORK &  A workspace for the procedure to utilize&  A complex array of size 12\\
		\hline
		LWORK&  Dimension of WORK &  12\\
		\hline
		RWORK&  A real workspace of size 2N &  A double array of size 12\\
		\hline
		INFO &  An integer containing $0$ if the algorithm was able to compute every eigenvalue &  A pointer to an integer variable\\
		\hline
	\end{tabular}
	\end{center}
	\caption{The ZGEEV's parameter list.}
\end{table}

\subsection{Symbolic Computation}

Symbolic computation is a vast topic, which deals with the problem of solving or manipulating mathematical expressions computationally. 

Back in \autoref{chapter:e3p}, we were faced with the problem of writing the function $\xi$ defined in \autoref{eq:circumscribed_circle_b} as a complex polynomial in the power format by replacing the sine and cosine functions with the identities given by  \autoref{eq:complex_trig_cos} and \autoref{eq:complex_trig_sin}.

As expected, computing the coefficients of that polynomial in terms of the E3P's instance by hand is very challenging; the expressions get too long, and it becomes humanly impossible not to make any mistake. 
For that reason, we resort to Symbolic computation for this task.

In practice, we utilized an external library for Python called SymPy (see \citeonline{sympy} for more information).
This tool can create expressions using arithmetic operators on predefined symbols, numbers, and other expressions. It can also convert expressions into polynomials in the power format, and output them directly into C code. Using these features, we can write $\xi(\theta)(e^{i\theta})^6$ as a polynomial by replacing the sine and cosine functions with expressions for the identities given by  \autoref{eq:complex_trig_cos} and \autoref{eq:complex_trig_sin}, and then import it into our C++ implementation of \autoref{algoritmo:e3p} by printing the polynomial's list of coefficients as C code.

%This can be done by creating a composition of an expression for $\xi(\theta)(e^{i\theta})^6$ with the expressions for $\cos(\theta)$ and $\sin(\theta)$ defined by \autoref{eq:complex_trig_cos} and \autoref{eq:complex_trig_sin}.
%After that, running a command, we can ask SymPy to transform that expression into a polynomial informing it that its variable is $z=e^{i\theta}$. Finally, very conveniently for us, SymPy has a function that outputs expressions directly as C code, which can be used to import the polynomial into our implementation of \autoref{algoritmo:e3p}.


\section{A greedy algorithm}

In \citeonline{church:1974}, a simple greedy algorithm was introduced to compare the results obtained by the other algorithms developed by them. 
Here we introduce a very similar algorithm for both MCE and MCER with the intention of using it in the development of a sufficient condition to skip non-optimal solutions.

Let $(\Pp, \Ww, \Rr)$ be an instance of MCE or MCER. Then, at the $j$-th iteration of the algorithm we choose the solution for the first ellipse. Considering $Z_j \subset \Pp$ as the set of uncovered points before the $j$-th iteration. Then, we set the solution for the $j$-th ellipse, as the solution of an instance of MCE-1 or MCER-1 with demand points $Z_j$.
That is the same as choosing, among all the possibilities in the $j$-th ellipse's CLS, the solution which maximizes the weight of covered points in $Z_j$. This algorithm can be implemented, such that it takes $\bigO(n^4m)$ operations to construct a solution.



\section{Some details and improvements}\label{section:improvements}

To achieve the results that are shown later in this chapter, an efficient implementation of \autoref{algoritmo:mce} and \autoref{algoritmo:mcer} had to be done. Just translating those algorithms into a programming language was not enough to obtain solutions for every instance previously published in \citeonline{andreta}.
Therefore, we present here, some improvements that can be applied to the implementation of those algorithms, which can result in a significant improvement in performance, especially in terms of CPU time.

In both algorithms, a subroutine to construct an ellipse's CLS is called inside the backtracking routine. This can potentially make the same combination of points be considered multiple times.
To avoid this unnecessary computation, points that have already been considered for an ellipse's CLS can be stored, and only new combinations can then be taken into account each time the CLS subroutine is called. 

Another improvement that can be made in the construction of an ellipse's CLS is the elimination of redundant solutions.
Let $(Q, \Theta)$ and $(Q', \Theta')$ be two solutions of MCER. If, for any $j \in \{1, \dots, m\}$, we have $\Pp \cap E_j(q_j', \theta_j') \subset \Pp \cap E(q_j, \theta_j)$, then we can for sure dismiss solution $(Q', \Theta')$.
In our implementation, we use the same tree-like data structure as the one described by \citeonline{andreta} to only keep solutions that are not redundant.

Also, to avoid solving the subproblem E3P in the algorithm for MCER, we can check if the maximum distance between any of the three points is greater than $2a$, where $a$ is an ellipse's semi-major. This is useful because calling \autoref{algoritmo:e3p} for every triplet of points of an instance of MCER can be very expensive.

The last implementation improvement was obtained by keeping an upper-bound, which can be used to skip solutions that are surely non-optimal.
When the backtracking algorithm has the location for the first $j$ ellipses fixed, we want to have an upper-bound for the best solution that can still be found by choosing the location of the $m-j$ remaining ellipses.
Let us consider the case for the MCER's algorithm (the MCE's case is analogous). 

Given an instance $(\Pp, \Ww, \Rr)$ of MCER, assume that the first $j$ ellipses are fixed at the locations $(q_1, \theta_1); \dots; (q_m, \theta_m)$. We will use the solution returned by the greedy algorithm defined earlier as an upper-bound for the solution considering only the $m-j$ remaining ellipses. 
Let 
\begin{equation*}
Z_j=\Pp\setminus \cup_{k=1}^j E_k(q_k, \theta_k),
\end{equation*}
be the points from the demand set which are not covered by the first $j$ ellipses, and $(q_{j+1}', \dots, q_m')\in \R^{2(m-j)}$, $(\theta_{j+1}, \dots, \theta_m) \in [0, \pi)^{m-j}$ be the solution returned by the greedy algorithm for the instance $(Z_j, \{w_k \in \Ww: p_k \in Z_j\}, \{(a_{j+1}, b_{j+1}); \dots; (a_m, b_m)\})$ of MCER. That is, the instance with demand points that are not covered by the first $j$ ellipses. Then, let $OPT_j$ be the value of the best solution for the instance $(\Pp, \Ww, \Rr)$ with the first $j$ ellipses with location fixed at $(q_1, \theta_1); \dots; (q_j, \theta_j)$. Then, we have the following inequality

\begin{equation}
OPT_j \le w\left(\bigcup_{k=1}^{j} \Pp \cap E_k(q_k, \theta_k)\right) + w\left(\bigcup_{k=j+1}^{m} \Pp \cap E_k(q_k', \theta_k')\right)
\end{equation}
This upper-bound for $OPT_j$ can then be used in the backtracking process to skip solutions that are not better. Let $OPT_{lo}$ be a lower bound for the optimal solution, then the inequality
\begin{equation}
\label{eq:upper-bound}
w\left(\bigcup_{k=1}^{j} \Pp \cap E_k(q_k, \theta_k)\right) +w\left(\bigcup_{k=j+1}^{m} \Pp \cap E_k(q_k', \theta_k')\right) \le OPT_{lo},
\end{equation}
defines a sufficient condition for us to dismiss every solution which have the location of the first $j$ ellipses fixed at $(q_1, \theta_1); \dots; (q_m, \theta_m)$. In practice, we can use the value of the best solution found so far by the backtracking process as the lower-bound $OPT_{lo}$.

It is worth pointing out that these improvement suggestions do not have an effect in a possible worst case scenario. We are adopting them in our implementation because they showed good results in practice.
For example, without taking the suggestion given by \autoref{eq:upper-bound}, \autoref{algoritmo:mcer-k} takes nine seconds to obtain an optimal solution for instance AB060, going through \num{336494451} solutions.
In \autoref{tab:mcer-results-ab1}, we show the results of \autoref{algoritmo:mcer-k} implemented with all the improvement suggestions given here; for the instance AB060, the algorithm takes less than one second to return an optimal solution, and evaluates only \num{1809} solutions.

\section{Results for known instances}

In this section, we present the results of \autoref{algoritmo:mce-k} and \autoref{algoritmo:mcer-k} for the instances CM1,CM2, CM4, CM5, CM7, CM8 proposed by \citeonline{canbolat}, and for the instances CM3,CM6,CM9 and AB001-AB120 proposed by \citeonline{andreta}.

For each instance, we display the selected ellipses and the income of the found optimal solution. 
We also display some performance metrics with the intention of giving an idea of how much computation had to be done for the algorithms to find an optimal solution. These metrics are: 
the CLS size of every ellipse, the number of nodes in the backtracking tree, the number of leaves corresponding to a solution in the backtracking tree, the CPU time spent on constructing the CLSs, and the total CPU time.
For the algorithms for MCER, we also have a column for the number of E3P subproblems that were solved, not counting the triplet of points which are dismissed by the improvements suggestions given in \autoref{section:improvements}.


\subsection{MCE-$k$}

In \autoref{tab:mce-results-cm}, the results for instances CM1-CM9 are shown. 
The algorithm proposed here showed great results as it was able to obtain optimal solutions in less than one second for every one of the instances CM1-CM9.
Even though the experiments were run in a different environment, we can still say that this is a great improvement compared with the results from \citeonline{andreta}. For example, to obtain an optimal solution for the instance CM9, the method proposed by \citeonline{andreta} took more than thirty minutes.
In \autoref{tab:mce-results-ab1} and \autoref{tab:mce-results-ab2}, we present the results for instances AB001-AB120. The only instance that our algorithm took more than one second to return an optimal solution was AB120, which it took 1.08 second.

In practice, at least for these instances, the bound $\bigO(n^{3m})$ for the algorithm for MCE-$k$ seems to be loose. In instance CM9, for example, this bound says that the number of leaves corresponding to a solution in the backtracking tree should be close to $n^{3m} = (10^2)^{3\times 3} = 10^{18}$, which is very far from the actual number of $649$ such leaves obtained in practice.
This is also the case for the size of the CLSs, which are all very far away from its $\bigO(n^2)$ bound. The greatest CLS size observed was $174$ for instance CM9, which is still very far away from $n^2$, which in this case is $10^4$.
\begin{table}
	\begin{center}
		\resizebox{\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrr|}
				\hline
				\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{5}{c|}{Performance metrics}\\
				\hline
				
				%%% Second line of header
				
				\multirow{2}{*}{Name} & 
				\multirow{2}{*}{$n$} & 
				\multirow{2}{*}{$m$} & 
				\multirow{2}{*}{$k$} & 
				Selected & 
				\multirow{2}{*}{Income} & 
				CLS size&
				\multicolumn{2}{c}{Backtracking Tree} & 
				\multicolumn{2}{c|}{\centering CPU Time (s)}\\
				& & & & \centering Ellipses & & $|S_k|$ & \# nodes & \# sol. leaves & CLS-MCE & Total\\
				\hline
				\input{tex/table_mcek_cm.tex}
				\hline
				
			\end{tabular}
		}
		\caption{Solutions of MCE-$k$ for instances CM1-CM9.}
		\label{tab:mce-results-cm}
	\end{center}
\end{table}

\begin{table}
	\begin{center}
		\resizebox{0.85\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrr|}
				\hline
				\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{5}{c|}{Performance metrics}\\
				\hline
				
				%%% Second line of header
				
			\multirow{2}{*}{Name} & 
			\multirow{2}{*}{$n$} & 
			\multirow{2}{*}{$m$} & 
			\multirow{2}{*}{$k$} & 
			Selected & 
			\multirow{2}{*}{Income} & 
			CLS size&
			\multicolumn{2}{c}{Backtracking Tree} & 
			\multicolumn{2}{c|}{\centering CPU Time (s)}\\
			& & & & \centering Ellipses & & $|S_k|$ & \# nodes & \# sol. leaves & CLS-MCE & Total\\
				\hline
				\input{tex/table_mcek_ab_1.tex}
				\hline
				
			\end{tabular}
		}
		\caption{Solutions of MCE-$k$ for instances AB001-AB060.}
		\label{tab:mce-results-ab1}
	\end{center}
\end{table}

\begin{table}
	\begin{center}
		\resizebox{0.85\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrr|}
				\hline
				\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{5}{c|}{Performance metrics}\\
				\hline
				
				%%% Second line of header
\multirow{2}{*}{Name} & 
\multirow{2}{*}{$n$} & 
\multirow{2}{*}{$m$} & 
\multirow{2}{*}{$k$} & 
Selected & 
\multirow{2}{*}{Income} & 
CLS size&
\multicolumn{2}{c}{Backtracking Tree} & 
\multicolumn{2}{c|}{\centering CPU Time (s)}\\
& & & & \centering Ellipses & & $|S_k|$ & \# nodes & \#sol. leaves & CLS-MCE & Total\\
				\hline
				\input{tex/table_mcek_ab_2.tex}
				\hline
				
			\end{tabular}
		}
		\caption{Solutions of MCE-$k$ for instances AB061-AB120.}
		\label{tab:mce-results-ab2}
	\end{center}
\end{table}

\subsection{MCER-$k$}

Two methods for MCER-$k$ were developed in \citeonline{andreta}: a deterministic method using global optimization, and a heuristic-stochastic method also using a global optimization, but taking the first found solution as a global optimizer.
The deterministic method could not find solutions for every instance within a specified time limit, however, comparing with the results of our algorithm, which are displayed in \autoref{tab:mcer-results-cm} for instances CM1-CM9, and in \autoref{tab:mcer-results-ab1} and \autoref{tab:mcer-results-ab2} for isntances AB001-AB120, we could observe that the heuristic method did find an optimal solution for every instance.

In general, our algorithm took much lower CPU time to return an optimal solution when compared with the heuristic method developed by \citeonline{andreta}. For example, for instance, CM9 it ran for more than six hours, while our implementation of \autoref{algoritmo:mcer-k} obtained an optimal solution in less than five seconds. 
One peculiarity that can be observed in the results for these instances is that the time spent in the backtracking phase is always very small, especially if compared to the time spent in the construction of the CLSs. 

As it was said for the results of MCE-$k$, in practice, the bounds for the CLS size and the number of operations taken by the algorithm is very loose.
Notice that, the greatest CLS size was $701$ obtained for instances CM7-CM9, which is very far away from its bound $\bigO(n^3)$, which in this case is $10^6$. By \autoref{th:mcer}, for MCER, the bound for the number of computations that \autoref{algoritmo:mcer-k} takes is $\bigO(n^{4m})$. In instance AB120, for example, the size of the backtracking tree is only $7102$, which is way lower than $100^{15}$.

\begin{table}
	\begin{center}
		\resizebox{\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrrr|}
				\hline
				\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{6}{c|}{Performance metrics}\\
				\hline
				
				%%% Second line of header
				
				\multirow{2}{*}{Name} & 
				\multirow{2}{*}{$n$} & 
				\multirow{2}{*}{$m$} & 
				\multirow{2}{*}{$k$} & 
				Selected & 
				\multirow{2}{*}{Income} & 
				CLS size&
				\# E3P&
				\multicolumn{2}{c}{Backtracking Tree} & 
				\multicolumn{2}{c|}{\centering CPU Time (s)}\\
				& & & & \centering Ellipses & & $|S_k|$ & subproblems & \# nodes & \#sol leaves & CLS-MCER & Total\\
				
				%%%
				%%%
				
				\hline
				\input{tex/table_mcerk_cm.tex}
				\hline
				
			\end{tabular}
		}
		\caption{Solutions of MCER-$k$ for instances CM1-CM9.}
		\label{tab:mcer-results-cm}
	\end{center}
\end{table}

\begin{table}
	\begin{center}
		\resizebox{\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrrr|}
				\hline
\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{6}{c|}{Performance metrics}\\
\hline

%%% Second line of header

\multirow{2}{*}{Name} & 
\multirow{2}{*}{$n$} & 
\multirow{2}{*}{$m$} & 
\multirow{2}{*}{$k$} & 
Selected & 
\multirow{2}{*}{Income} & 
CLS size&
\# E3P&
\multicolumn{2}{c}{Backtracking Tree} & 
\multicolumn{2}{c|}{\centering CPU Time (s)}\\
& & & & \centering Ellipses & & $|S_k|$ & subproblems & \# nodes & \#sol leaves & CLS-MCER & Total\\
				
				%%%
				%%%
				
				
				\hline
				\input{tex/table_mcerk_ab_1.tex}
				\hline
					
			\end{tabular}
 		}
 	\caption{Solutions of MCER-$k$ for instances AB001-AB060.}
 	\label{tab:mcer-results-ab1}
	\end{center}
\end{table}

\begin{table}
	\begin{center}
		\resizebox{\textwidth}{!}{%
			
			\begin{tabular}{|cccc|cr|crrrrr|}
				\hline
\multicolumn{4}{|c|}{Instance} & \multicolumn{2}{c|}{Optimal Solution} & \multicolumn{6}{c|}{Performance metrics}\\
\hline

%%% Second line of header

\multirow{2}{*}{Name} & 
\multirow{2}{*}{$n$} & 
\multirow{2}{*}{$m$} & 
\multirow{2}{*}{$k$} & 
Selected & 
\multirow{2}{*}{Income} & 
CLS size&
\# E3P&
\multicolumn{2}{c}{Backtracking Tree} & 
\multicolumn{2}{c|}{\centering CPU Time (s)}\\
& & & & \centering Ellipses & & $|S_k|$ & subproblems & \# nodes & \#sol leaves & CLS-MCER & Total\\
				
				%%%%
				%%%%
				
				
				\hline
				\input{tex/table_mcerk_ab_2.tex}
				\hline
				
			\end{tabular}
		}
		\caption{Solutions of MCER-$k$ for instances AB061-AB120.}
		\label{tab:mcer-results-ab2}
	\end{center}
\end{table}

\section{Other instances}

In this section, we describe some new instances which were created with the intention of further analyzing the algorithms developed by our work.

To generate the demand set, for each point, we had its coordinates follow a normal distribution $\mathcal{N}(0, 1)$. This way, most of the points are expected to be near the origin, making optimal solutions cover bigger portions of the demand set when compared to the previously analyzed instances.
We constructed eight instances with $n=100$ and $m=8$, and generated the ellipse's shapes uniformly randomly in the interval $[0.5, 1.5]$, setting the cost for the $i$-th ellipse as $c_i=10 \times a_i\times b_i$, and the weight of every point as its distance to the origin.
